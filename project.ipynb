{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c3d2aa-e951-4105-87f8-df5b1d52e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/anaconda3/envs/hpq/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.text as text\n",
    "from tensorflow.keras import layers, losses, metrics, initializers, regularizers\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "\n",
    "# In this cell I import all of the libraries that I will be using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48c2560-0ca9-475c-8cab-0a5a841b40a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unncecesary data from dataset, and remove very unclear values\n",
    "data = pd.concat([pd.read_csv(\"data/full_dataset/goemotions_1.csv\"), pd.read_csv(\"data/full_dataset/goemotions_2.csv\"), pd.read_csv(\"data/full_dataset/goemotions_3.csv\")])\n",
    "data = data.drop(data[data.example_very_unclear == True].index)\n",
    "data = data.drop(data[data.neutral == True].index)\n",
    "data = data.drop(labels=['id', \"author\", \"subreddit\", \"link_id\", \"parent_id\", \"created_utc\", \"rater_id\", \"example_very_unclear\", \"neutral\"], axis=1)\n",
    "\n",
    "data.sum(axis=1, numeric_only=True) >1\n",
    "data = data[data.sum(axis=1, numeric_only=True) == 1]\n",
    "data = data.reset_index().dropna()\n",
    "\n",
    "# In this cell I remove unncecesary data from dataset, and remove very unclear values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1542e637-5a55-4afe-96ec-7492ecc53af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5097.0, 12.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_median = np.median([len(words.split()) for words in data[\"text\"]])\n",
    "len(data) / num_words_median, num_words_median\n",
    "\n",
    "# In this cell I calculate some staticstics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf55754e-710e-42a8-bfc1-8a167ef715bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " index                           0\n",
       " text              That game hurt.\n",
       " admiration                      0\n",
       " amusement                       0\n",
       " anger                           0\n",
       " annoyance                       0\n",
       " approval                        0\n",
       " caring                          0\n",
       " confusion                       0\n",
       " curiosity                       0\n",
       " desire                          0\n",
       " disappointment                  0\n",
       " disapproval                     0\n",
       " disgust                         0\n",
       " embarrassment                   0\n",
       " excitement                      0\n",
       " fear                            0\n",
       " gratitude                       0\n",
       " grief                           0\n",
       " joy                             0\n",
       " love                            0\n",
       " nervousness                     0\n",
       " optimism                        0\n",
       " pride                           0\n",
       " realization                     0\n",
       " relief                          0\n",
       " remorse                         0\n",
       " sadness                         1\n",
       " surprise                        0\n",
       " Name: 0, dtype: object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def array_to_label(array):\n",
    "    for i in range(0, 27):\n",
    "        if array[i] == 1:\n",
    "            return data.columns[i]\n",
    "\n",
    "def array_to_num(array):\n",
    "    for i in range(1, 29):\n",
    "        if array[i] == 1:\n",
    "            return i - 1\n",
    "array_to_label(data.iloc[0]), data.iloc[0], \n",
    "\n",
    "# In this cell I define some functions which allow me to convert the formats in which the results are being stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae860cc0-93d8-4265-a9c2-ee486da67d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48931 12233\n"
     ]
    }
   ],
   "source": [
    "training_size = floor(len(data)*0.8)\n",
    "testing_data = data.iloc[training_size:]\n",
    "training_data = data.iloc[:training_size]\n",
    "print(len(training_data), len(testing_data))\n",
    "\n",
    "# In this cell I split the data into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485fa39b-9d25-4d34-95f4-9276c14275f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 13:37:32.747214: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-25 13:37:32.760392: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "vectorization_layer = layers.TextVectorization(\n",
    "    max_tokens=30000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=30,\n",
    ")\n",
    "\n",
    "# In this cell I create a layer which can be used to vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a1aff3-47e5-4e39-b36a-163029390e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = training_data.text\n",
    "type(training_text)\n",
    "vectorization_layer.adapt(training_text)\n",
    "\n",
    "# In this cell I adapt the vectorization layer to the vocabulary used in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98abde6-f552-416d-919a-aac2dc3f375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorization_layer(text)\n",
    "\n",
    "# In this cell I create a function which makes it easier to use the vectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3b81fc-7902-4090-8087-b8aee43301f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 30), dtype=int64, numpy=\n",
       " array([[12117, 15676,  7658, 12795,   212, 14313,   764,  8638,  4806,\n",
       "             8,    59,   216,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]])>,\n",
       " '\"Sponge Blurb Pubs Quaw Haha GURR ha AAa!\" finale is too real')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_text = training_text.iloc[3]\n",
    "vectorize_text(first_text), first_text\n",
    "\n",
    "# In this cell I test the text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3c68c6-d571-474a-940e-ed0d4f3b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(zip(list(range(20000)), vectorization_layer.get_vocabulary()))\n",
    "\n",
    "# In this cell I export the vocabulary of the vectorization layer to a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74c55760-1f10-43c0-a554-c802a5bc4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48931/48931 [01:32<00:00, 530.35it/s]\n",
      "100%|██████████| 12233/12233 [00:21<00:00, 569.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "preprocessed_training_data = training_data.text.progress_map(lambda x: vectorize_text(x))\n",
    "preprocessed_testing_data = testing_data.text.progress_map(lambda x: vectorize_text(x))\n",
    "\n",
    "# In this cell I apply the vectorization layer to both the testing and the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f94ae2f-fc71-40af-91da-34217360116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_data = tf.stack(list(preprocessed_training_data))\n",
    "preprocessed_testing_data = tf.stack(list(preprocessed_testing_data))\n",
    "\n",
    "# In this cell I convert the training and testing data to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a7ab0-2704-4616-a51a-b76b5975b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sepcnn_model(hp):\n",
    "    if hp:\n",
    "        filters = hp.Int(\"filters\", min_value=20, max_value=100, step=20)\n",
    "        blocks = hp.Int(\"blocks\", min_value=1, max_value=4, step=1)\n",
    "        learning_rate = hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
    "        dropout_rate = hp.Float(\"dropout_rate\", min_value=0.2, max_value=0.5, step=0.1)\n",
    "        kernel_size = hp.Int(\"kernel_size\", min_value=2, max_value=6)\n",
    "        pool_size = hp.Int(\"pool_size\", min_value=2, max_value=6)\n",
    "\n",
    "\n",
    "        conv_activations = hp.Choice(\"conv_activations\", [\"tanh\", \"relu\"])\n",
    "\n",
    "        embedding_dim = hp.Int(\"embedding_dim\", min_value=50, max_value=300, step=50)\n",
    "        \n",
    "    sepcnn_model = tf.keras.Sequential()\n",
    "    sepcnn_model.add(layers.Embedding(30001, output_dim=embedding_dim, input_length=50))\n",
    "    for i in range(blocks-1):\n",
    "        sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "        sepcnn_model.add(layers.SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation=conv_activations,\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        sepcnn_model.add(layers.SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation=conv_activations,\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        sepcnn_model.add(layers.MaxPooling1D(pool_size=pool_size, padding=\"same\"))\n",
    "    sepcnn_model.add(layers.SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation=conv_activations,\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "    sepcnn_model.add(layers.SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation=conv_activations,\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    sepcnn_model.add(layers.GlobalAveragePooling1D())\n",
    "    sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    dense_activation = hp.Choice(\"dense_activations\", [\"tanh\", \"relu\"])\n",
    "    first_dense_units = hp.Int(\"first_dense_units\", min_value = 20, max_value = 370, step=50)\n",
    "    sepcnn_model.add(layers.Dense(first_dense_units, activation=dense_activation))\n",
    "    sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    sepcnn_model.add(layers.Dense(27, activation=\"softmax\"))\n",
    "\n",
    "    \n",
    "    sepcnn_model.compile(loss=losses.CategoricalCrossentropy(from_logits=False),\n",
    "                         optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         metrics=[\"accuracy\", tf.keras.metrics.CategoricalCrossentropy(from_logits=False)])\n",
    "    return sepcnn_model\n",
    "\n",
    "# In this cell I define the sepcnn_model to be used with the hyperparameter tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2fc8f-3b81-4557-b81f-b8eab5b31169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sepcnn_for_training(embedding_dim, filters, blocks, learning_rate, dropout_rate, kernel_size, pool_size, conv_activations, dense_activation, first_dense_units, second_dense_units):\n",
    "    sepcnn_model = tf.keras.Sequential()\n",
    "    sepcnn_model.add(layers.Embedding(30001, output_dim=embedding_dim, input_length=30))\n",
    "    for i in range(blocks-1):\n",
    "        sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "        sepcnn_model.add(layers.SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation=conv_activations,\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        sepcnn_model.add(layers.SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation=conv_activations,\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        sepcnn_model.add(layers.MaxPooling1D(pool_size=pool_size, padding=\"same\"))\n",
    "    sepcnn_model.add(layers.SeparableConv1D(filters=filters * 2,\n",
    "                          kernel_size=kernel_size,\n",
    "                          activation=conv_activations,\n",
    "                          bias_initializer='random_uniform',\n",
    "                          depthwise_initializer='random_uniform',\n",
    "                          padding='same'))\n",
    "    sepcnn_model.add(layers.SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation=conv_activations,\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    sepcnn_model.add(layers.GlobalAveragePooling1D())\n",
    "    sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    #dense_activation = hp.Choice(\"dense_activations\", [\"tanh\", \"relu\"])\n",
    "    #first_dense_units = hp.Int(\"first_dense_units\", min_value = 20, max_value = 370, step=50)\n",
    "    sepcnn_model.add(layers.Dense(first_dense_units, activation=dense_activation))\n",
    "    sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    if second_dense_units > 0:\n",
    "        sepcnn_model.add(layers.Dense(second_dense_units, activation=dense_activation)),\n",
    "        sepcnn_model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    sepcnn_model.add(layers.Dense(27, activation=\"softmax\"))\n",
    "\n",
    "    \n",
    "    sepcnn_model.compile(loss=losses.CategoricalCrossentropy(from_logits=False),\n",
    "                         optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                         metrics=[\"accuracy\"])\n",
    "    return sepcnn_model\n",
    "\n",
    "# In this cell I define the sepCNN model for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c68be3-c082-4209-a62b-579b150b347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann_model(hp):\n",
    "\n",
    "    ann_model = tf.keras.Sequential([\n",
    "        layers.Embedding(30001, hp.Int(\"embedding_dim\", min_value=50, max_value=250, step=50), input_length=30),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(hp.Float(\"dropout_rate\", min_value = 0.05, max_value=0.4, step=0.05)),\n",
    "        layers.Dense(units=hp.Int(\"units\", min_value=32, max_value=512, step=32), activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])),\n",
    "        layers.Dropout(hp.Float(\"dropout_rate\", min_value = 0.05, max_value=0.4, step=0.05)),\n",
    "        layers.Dense(27, activation=\"softmax\"),\n",
    "    ])\n",
    "    ann_model.compile(loss=losses.CategoricalCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics = [\"accuracy\", tf.metrics.categorical_crossentropy])\n",
    "    return ann_model\n",
    "\n",
    "# In this cell I define the feed-forward model for hyperparameter tuning             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a9c80-bdfa-4701-bf25-2ad5ef53d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ann_model_for_training(embedding_dim, first_layer_units, second_layer_units, dropout_rate, activation):\n",
    "\n",
    "    ann_model = tf.keras.Sequential([\n",
    "        layers.Embedding(30001, embedding_dim, input_length=30),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(first_layer_units, activation),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(second_layer_units, activation),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(27, activation=\"softmax\"),\n",
    "    ])\n",
    "    ann_model.compile(loss=losses.CategoricalCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics = [\"accuracy\", tf.metrics.categorical_crossentropy])\n",
    "    return ann_model\n",
    "# In this cell I define the feed-forward model for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd5356f-3c47-4908-b93e-3882bf977e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = training_data.drop(labels=[\"text\"], axis=1, inplace=False)\n",
    "testing_labels = testing_data.drop(labels=[\"text\"], axis=1, inplace=False)\n",
    "\n",
    "# In this cell I separate the training and testing labels from the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9856dc-acbe-4f66-bb03-1b037cb8b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48931/48931 [00:00<00:00, 51662.39it/s]\n",
      "100%|██████████| 48931/48931 [00:01<00:00, 45976.46it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_training_labels = list(training_labels.progress_apply(lambda x: array_to_num(x), axis=1))\n",
    "preprocessed_training_labels\n",
    "\n",
    "processed_training_labels = list(training_labels.progress_apply(lambda x: array_to_label(x), axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e88087-846c-4658-be28-c4a5e685e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessed_training_data = tf.expand_dims(preprocessed_training_data, -1)\n",
    "#reprocessed_training_data_copy = tf.reshape(preprocessed_training_data, [55096, 50])\n",
    "#reprocessed_training_labels = tf.reshape(training_labels, [55096,1,28])\n",
    "#reprocessed_training_data_copy.shape, preprocessed_training_data.shape, training_labels.shape, preprocessed_training_labels.shape\n",
    "#testing_labels.shape, preprocessed_testing_data.shape\n",
    "preprocessed_testing_data_copy = tf.reshape(preprocessed_testing_data, [12233, 30])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f418b693-b4cc-4ecb-a790-92e53cdf1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_training_data_copy = tf.reshape(preprocessed_training_data, [48931, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcad4d-4b62-48c5-b30b-455b72e7c5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(48931, 27), dtype=int64, numpy=\n",
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training_labels = training_labels.drop(columns=\"index\")   \n",
    "training_labels_copy = tf.convert_to_tensor(training_labels)\n",
    "training_labels_copy\n",
    "\n",
    "# In this cell I convert the training labels to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f43cf8-4f3b-444f-b401-a35fa30593f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_training_labels_copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessed_training_labels_copy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mpreprocessed_training_labels_copy\u001b[49m, [\u001b[38;5;241m133081\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_training_labels_copy' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_training_labels_copy = tf.reshape(preprocessed_training_labels_copy, [133081, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010a462-7d01-4445-8fa6-ab01e11bc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "\n",
    "# In this cell I define the callbacks which stop the training early if the accuracy decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c48c02c4-79b1-4826-a432-175c93d94bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project sepcnn_dir_2/sepcnn_classifier_2/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from sepcnn_dir_2/sepcnn_classifier_2/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "sepcnn_tuner = kt.Hyperband(\n",
    "    build_sepcnn_model,\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory=\"sepcnn_dir_2\",\n",
    "    project_name=\"sepcnn_classifier_2\", \n",
    "    objective=\"val_accuracy\",\n",
    "    overwrite=False\n",
    ")\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ec573-be4c-4339-ba5e-6439f1bd33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcnn_tuner.search(\n",
    "    x=preprocessed_training_data_copy,\n",
    "    y=training_labels_copy,\n",
    "    epochs=15,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[stop_early], \n",
    "    batch_size=32\n",
    ")\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c30eea0-4ba1-4f37-980e-4db52b9ae39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps=sepcnn_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps.get(\"filters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2f49d-e2ed-4086-8c78-089ea90264ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sepcnn_model = build_sepcnn_for_training(\n",
    "    filters=60,\n",
    "    blocks=1,\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.25,\n",
    "    kernel_size=3,\n",
    "    pool_size=5,\n",
    "    conv_activations=\"relu\",\n",
    "    embedding_dim=200,\n",
    "    dense_activation=\"tanh\",\n",
    "    first_dense_units=100,\n",
    "    second_dense_units=50\n",
    ")\n",
    "\n",
    "# In this cell I build the final sepCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0aea4-7b35-42cd-9c01-30c3f39db97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1224/1224 [==============================] - 251s 204ms/step - loss: 3.0941 - accuracy: 0.0916 - val_loss: 3.0728 - val_accuracy: 0.0974\n",
      "Epoch 2/80\n",
      "1224/1224 [==============================] - 244s 199ms/step - loss: 3.0794 - accuracy: 0.0944 - val_loss: 3.0732 - val_accuracy: 0.0974\n",
      "Epoch 3/80\n",
      "1224/1224 [==============================] - 288s 235ms/step - loss: 3.0782 - accuracy: 0.0946 - val_loss: 3.0720 - val_accuracy: 0.0974\n",
      "Epoch 4/80\n",
      "1224/1224 [==============================] - 363s 297ms/step - loss: 3.0774 - accuracy: 0.0966 - val_loss: 3.0729 - val_accuracy: 0.0974\n",
      "Epoch 5/80\n",
      "1224/1224 [==============================] - 358s 292ms/step - loss: 3.0767 - accuracy: 0.0969 - val_loss: 3.0735 - val_accuracy: 0.0974\n"
     ]
    }
   ],
   "source": [
    "sepcnn_history = final_sepcnn_model.fit(\n",
    "    x=preprocessed_training_data_copy,\n",
    "    y=training_labels_copy,\n",
    "    epochs=80,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# In this cell I train the final sepCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007a1e6-ef6d-46d1-8678-157ca4874419",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sepcnn_model2.evaluate(x=preprocessed_testing_data_copy, y=preprocessed_testing_labels)\n",
    "\n",
    "# In this cell I test the final sepCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391cd7f-6941-4e47-b7e0-bb5e2069fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_history = ann_model.fit(\n",
    "    x=preprocessed_training_data_copy,\n",
    "    y=training_labels_copy,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118380b0-8e64-4d6c-8155-b604bec4304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_ann_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"hpq_ann\",\n",
    "    overwrite=\"true\"\n",
    ")\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# In this cell I define the parameters for the hyperparameter tuning for the feed-forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccea4bf7-1d71-4e74-9d12-ac672f563ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ann = tf.keras.models.load_model(\"ann\")\n",
    "trained_cnn = tf.keras.models.load_model(\"cnn\")\n",
    "\n",
    "# In this cell I load the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01c989-dd0c-4397-9bf1-da6302b11f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(preprocessed_training_data_copy, training_labels_copy, epochs=50, validation_split=0.15, callbacks=[stop_early])\n",
    "optimal_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "                                                                                                                \n",
    "# In this cell I search for the optimal hyperparameters                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2cf4d-65dc-4d8a-b520-f19f3f531325",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# In this cell I return the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b977de-f1c7-4650-af56-e0b69b35931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = build_ann_model_for_training(\n",
    "    embedding_dim=150, \n",
    "    dropout_rate=0.2, \n",
    "    first_layer_units=128, \n",
    "    second_layer_units=64, \n",
    "    activation=\"tanh\")\n",
    "\n",
    "# In this cell I build the final feed-forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3d10a-1c79-4807-b048-dd94d14dcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.fit(\n",
    "    x=preprocessed_training_data_copy, \n",
    "    y=training_labels_copy, \n",
    "    epochs=10, \n",
    "    validation_split=0.15, \n",
    "    callbacks=callbacks)\n",
    "\n",
    "# In this cell I train the final feed-forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf3fdc-d69d-44e8-9cdc-8014cf19474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.evaluate(preprocessed_testing_data_copy, preprocessed_testing_labels)\n",
    "\n",
    "# In this cell I test the final feed-forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966fcf36-9460-4612-8d6b-2cb004d41f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.save(\"ann\")\n",
    "\n",
    "# In this cell I save the feed-forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ca721f-1f85-4e1f-ab51-070b3c954911",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_testing_labels = tf.convert_to_tensor(testing_labels.drop(columns=\"index\"))\n",
    "\n",
    "# In this cell I convert the testing labels to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7536087e-83cb-4a54-87f5-05763abc4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 2s 5ms/step - loss: 2.6586 - accuracy: 0.3461\n",
      "383/383 [==============================] - 2s 5ms/step - loss: 2.3986 - accuracy: 0.3718 - categorical_crossentropy: 2.3986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.6585614681243896, 0.3461129665374756],\n",
       " [2.3985812664031982, 0.37178125977516174, 2.3985812664031982])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_cnn.evaluate(x=preprocessed_testing_data_copy, y=preprocessed_testing_labels), trained_ann.evaluate(x=preprocessed_testing_data_copy, y=preprocessed_testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673ab0a6-818c-4f43-b1be-b753a35fd637",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessed_testing_data_copy \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_testing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12233\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hpq/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/hpq/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor)."
     ]
    }
   ],
   "source": [
    "preprocessed_testing_data_copy = tf.reshape(preprocessed_testing_data, [12233, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb48732-f1a6-49d2-895f-679ecb06fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcnn_model = build_sepcnn_model(embedding_dim=200, filters=32, blocks=2, dropout_rate=0.2, kernel_size=3, pool_size=3, learning_rate=1e-3)\n",
    "sepcnn_model.fit(\n",
    "    x=preprocessed_training_data_copy,\n",
    "    y=training_labels_copy,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e7c63-0b79-49fc-a23f-82283e44bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepcnn_model.evaluate(x=preprocessed_testing_data_copy,\n",
    "              y=preprocessed_testing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea4e4f0d-a962-4263-82dc-34e14e375c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - 2s 5ms/step - loss: 2.6586 - accuracy: 0.3461\n",
      "383/383 [==============================] - 3s 7ms/step - loss: 2.3986 - accuracy: 0.3718 - categorical_crossentropy: 2.3986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.6585614681243896, 0.3461129665374756],\n",
       " [2.3985812664031982, 0.37178125977516174, 2.3985812664031982])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_cnn.evaluate(x=preprocessed_testing_data_copy,\n",
    "              y=preprocessed_testing_labels), trained_ann.evaluate(x=preprocessed_testing_data_copy,\n",
    "              y=preprocessed_testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65217cec-1c1c-434c-82d1-5aa71b6f5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_data_copy, preprocessed_training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ef35eb-e958-4e0b-a75d-ebe58b3d0e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
       "       'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
       "       'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
       "       'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n",
       "       'relief', 'remorse', 'sadness', 'surprise'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data.columns.drop([\"text\", \"index\"])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4398e25-92e5-4c11-a0e9-2b011c9f8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sepcnn_model2.save(\"actualfinal_sepcnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1892f-c8b0-4a86-a3f7-cf068586cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sepcnn_model2.save(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c474d-efaf-4e7e-8d53-90c5c0b51d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_loaded = tf.keras.models.load_model(\"final_sepcnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69488d98-a508-42ee-933b-cdaef452214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9cc96-18cd-4a90-999e-9801890acd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_loaded.evaluate(\n",
    "    x=preprocessed_testing_data_copy,\n",
    "    y=preprocessed_testing_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "375376c6-304b-44dc-bdf6-104647da5a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 30, 200)           6000200   \n",
      "                                                                 \n",
      " separable_conv1d_2 (Separab  (None, 30, 40)           8640      \n",
      " leConv1D)                                                       \n",
      "                                                                 \n",
      " separable_conv1d_3 (Separab  (None, 30, 40)           1760      \n",
      " leConv1D)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 40)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               4100      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 27)                1377      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,021,127\n",
      "Trainable params: 6,021,127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2147d7d9-800b-4644-a539-a933e2c79d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approval': 0.10927341878414154,\n",
       " 'confusion': 0.09982244670391083,\n",
       " 'curiosity': 0.08398647606372833,\n",
       " 'annoyance': 0.059897832572460175,\n",
       " 'realization': 0.057530708611011505,\n",
       " 'sadness': 0.05199885368347168,\n",
       " 'excitement': 0.04941577464342117,\n",
       " 'admiration': 0.04929938167333603,\n",
       " 'disapproval': 0.0482577383518219,\n",
       " 'disappointment': 0.04453521966934204,\n",
       " 'surprise': 0.03903232514858246,\n",
       " 'anger': 0.03443717211484909,\n",
       " 'joy': 0.029241276904940605,\n",
       " 'disgust': 0.028340954333543777,\n",
       " 'love': 0.028020750731229782,\n",
       " 'amusement': 0.025000836700201035,\n",
       " 'embarrassment': 0.021047083660960197,\n",
       " 'caring': 0.018760673701763153,\n",
       " 'optimism': 0.01589212194085121,\n",
       " 'fear': 0.015567841939628124,\n",
       " 'nervousness': 0.015440806746482849,\n",
       " 'remorse': 0.01499853003770113,\n",
       " 'gratitude': 0.014954703859984875,\n",
       " 'pride': 0.014403589069843292,\n",
       " 'desire': 0.012958898209035397,\n",
       " 'relief': 0.011080865748226643,\n",
       " 'grief': 0.006803733296692371}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ann_predict(text):\n",
    "\n",
    "    vectorized = vectorize_text(text)\n",
    "    output = trained_ann.predict(vectorized)\n",
    "    return output\n",
    "\n",
    "#output = ann_predict(testing_data.text.iloc[76])\n",
    "output = ann_predict(\"\")\n",
    "output = tf.reshape(output, [27]).numpy().tolist()\n",
    "output_dict = dict(zip(labels, output))\n",
    "\n",
    "#highest_output = max(ouput_dict, key=output_dict.get)\n",
    "#output_dict, highest_output\n",
    "max(output_dict, key=output_dict.get), output_dict\n",
    "sorted(output_dict.values(), reverse=True)[:3]\n",
    "sorted_output_dict = dict(sorted(output_dict.items(), key=lambda item:item[1], reverse=True))\n",
    "sorted_output_dict#, testing_data.text.iloc[76]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0594476d-351f-47fa-8ab5-37a476ca5053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'fear': 0.42594608664512634,\n",
       "  'sadness': 0.21000884473323822,\n",
       "  'nervousness': 0.0901598408818245,\n",
       "  'embarrassment': 0.05420936271548271,\n",
       "  'disappointment': 0.04663660004734993,\n",
       "  'realization': 0.04185948148369789,\n",
       "  'remorse': 0.029163120314478874,\n",
       "  'grief': 0.02765926904976368,\n",
       "  'disgust': 0.02709975279867649,\n",
       "  'surprise': 0.015983998775482178,\n",
       "  'relief': 0.006445922423154116,\n",
       "  'annoyance': 0.003559886710718274,\n",
       "  'caring': 0.003495303215458989,\n",
       "  'approval': 0.002597693121060729,\n",
       "  'desire': 0.0022432920522987843,\n",
       "  'pride': 0.0021839728578925133,\n",
       "  'anger': 0.0018672236474230886,\n",
       "  'confusion': 0.0017620096914470196,\n",
       "  'disapproval': 0.0015190384583547711,\n",
       "  'excitement': 0.0011410551378503442,\n",
       "  'optimism': 0.0010064503876492381,\n",
       "  'curiosity': 0.0009361167321912944,\n",
       "  'amusement': 0.000843517598696053,\n",
       "  'joy': 0.0007211748161353171,\n",
       "  'gratitude': 0.0006001463043503463,\n",
       "  'admiration': 0.00018055542022921145,\n",
       "  'love': 0.00017029346781782806},\n",
       " \"Every time I see a something like, 'thanks for nothing, [NAME],' I mentally apologize to her, lol. \")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cnn_predict(text):\n",
    "\n",
    "    vectorized = vectorize_text(text)\n",
    "    output = trained_cnn.predict(vectorized)\n",
    "    return output\n",
    "\n",
    "output = cnn_predict(\"I feel scared\")\n",
    "output = tf.reshape(output, [27]).numpy().tolist()\n",
    "output_dict = dict(zip(labels, output))\n",
    "\n",
    "#highest_output = max(ouput_dict, key=output_dict.get)\n",
    "#output_dict, highest_output\n",
    "max(output_dict, key=output_dict.get), output_dict\n",
    "sorted(output_dict.values(), reverse=True)[:3]\n",
    "sorted_output_dict = dict(sorted(output_dict.items(), key=lambda item:item[1], reverse=True))\n",
    "sorted_output_dict, testing_data.text.iloc[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8396f78c-9153-4efe-9866-16b7460875b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_results():\n",
    "    results = []\n",
    "    predicted_values = ann_predict(preprocessed_testing_data_copy)\n",
    "    return predicted_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8621188c-e31f-43aa-89e2-b1156a0736c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annresults = ann_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "418561ff-dd0d-4711-83ce-4592b4ec084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(annresults).shape, preprocessed_testing_labels.numpy().shape\n",
    "count = 0\n",
    "for i in range(len(preprocessed_testing_labels.numpy())):\n",
    "    if np.argsort(annresults[i])[np.argmax(preprocessed_testing_labels.numpy()[i])] > 23:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5b9b3ae2-b671-4d5b-bd03-1f48ebb8f1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1883"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ddcb0364-182b-4d99-b1e7-836608ac5f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(15.)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray(annresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "15860738-b794-4841-a084-dac33431f5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12233, 27)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_predict(preprocessed_testing_data_copy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "89770794-c6e1-4e7e-b936-4f86714c0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(27,), dtype=int64, numpy=\n",
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_testing_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2254d-7e56-4884-a697-cc1e98b028e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in preprocessed_testing_data_copy:\n",
    "    print(ann_predict(tf.expand_dims(i, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7770e-0aa5-466b-8e4a-1925fb7674a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb6fb0-1118-4132-8ff6-51e81b214668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in preprocessed_testing_data_copy:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f82099-53e1-494d-a138-7df08261ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_testing_data_copy[1], array_to_label(preprocessed_testing_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcff866-4955-424a-9378-3897bb56b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.unique_with_counts(tf.map_fn(elems=training_labels_copy, fn=lambda x: array_to_label(list(x))))\n",
    "values = np.array([])\n",
    "for i in training_labels_copy.numpy():\n",
    "    values = np.append(values, array_to_label(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445be6d-0fa5-4c5c-be32-c46a6d55e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(values, return_counts=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b46297-6c69-4068-98f6-99559c5d2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(training_labels_copy, axis=0, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
